{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f7cd777-6d75-47b6-bba6-a44e317fb35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def github() -> str:\n",
    "    \"\"\"\n",
    "    This function returns a link to the GitHub repository containing solutions.\n",
    "\n",
    "    Returns:\n",
    "        str: The URL to the GitHub repository.\n",
    "    \"\"\"\n",
    "    return \"https://github.com/anasfathul/ECON481/blob/main/ECON481_HW2.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e116be4-3757-4f1d-af76-443e77cbca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed51f6ca-bc6f-4c4f-8e36-906f8a90ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "def simulate_data(seed: int = 481) -> tuple:\n",
    "    \"\"\"\n",
    "    This function returns 1000 simulated observations.\n",
    "    It take one argument for seed, an integer.\n",
    "    It return a tuple of two arrays,\n",
    "    (y, X), where y has a shape of 1000 x 1 and X has shape of 1000 x 3. \n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # Generate X matrix (1000 x 3)\n",
    "    X = rng.normal(0, np.sqrt(2), size=(1000, 3))\n",
    "\n",
    "    epsilon = rng.normal(0, 1, size=(1000, 1))\n",
    "\n",
    "    y = 5 + 3*X[:, 0:1] + 2*X[:, 1:2] + 6*X[:, 2:3] + epsilon\n",
    "\n",
    "    return y.reshape(-1, 1), X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0f04d1d-df26-4e96-bc40-50a008809373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "aeececca-a2b4-4cb2-b35b-8fb3661db99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2\n",
    "def estimate_mle(y, X):\n",
    "    \"\"\"\n",
    "    Estimates the MLE parameters for a linear model using optimization.\n",
    "    \n",
    "    Parameters:\n",
    "    - y: Dependent variable (1000 x 1 np.array).\n",
    "    - X: Independent variables (1000 x 3 np.array).\n",
    "    \n",
    "    Returns:\n",
    "    - A 4 x 1 np.array with the coefficients (beta_0, beta_1, beta_2, beta_3).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initial guess for the coefficients (including intercept)\n",
    "    initial_guess = np.zeros(X.shape[1] + 1)\n",
    "    # Minimize the negative log-likelihood\n",
    "    result = sp.optimize.minimize(fun=neg_log_likelihood, x0=initial_guess, args=(X, y), method = 'Nelder-Mead')\n",
    "    # Return the estimated parameters\n",
    "    return np.round(result.x, decimals = 0)\n",
    "\n",
    "def neg_log_likelihood(beta, X, y):\n",
    "    \"\"\"\n",
    "    Compute the negative log-likelihood for a linear regression model.\n",
    "    \n",
    "    Parameters:\n",
    "    - beta: Coefficients (including intercept) for the linear model as a numpy array.\n",
    "    - X: Independent variables as a numpy array (with an intercept column if applicable).\n",
    "    - y: Dependent variable as a numpy array.\n",
    "    \n",
    "    Returns:\n",
    "    - The negative log-likelihood of the linear regression model given the data.\n",
    "    \"\"\"\n",
    "    y_pred = np.hstack((np.ones(X.shape[0]).reshape(-1,1), X)) * beta  # Using matrix multiplication for predictions\n",
    "    \n",
    "    # Calculate residuals\n",
    "    residuals = y - y_pred\n",
    "    \n",
    "    # Estimate the variance of the residuals\n",
    "    sigma2_hat = np.sum(residuals**2) / len(y)\n",
    "    \n",
    "    # Compute the negative log-likelihood\n",
    "    nll = len(y) / 2 * np.log(2 * np.pi * sigma2_hat) + np.sum(residuals**2) / (2 * sigma2_hat)\n",
    "    \n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "72dcef13-6e9b-4b9e-af71-d54744f69446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 3., 2., 6.])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z, t = simulate_data(11)\n",
    "estimate_mle(z, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e0df9680-d308-49b3-a358-ba1f569870fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3\n",
    "def sum_of_squared_residuals(beta, X, y):\n",
    "    \"\"\"\n",
    "    Calculate the sum of squared residuals (SSR).\n",
    "    \n",
    "    Parameters:\n",
    "    - beta: np.array, the model coefficients including the intercept.\n",
    "    - X: np.array, the independent variables (observations x features).\n",
    "    - y: np.array, the dependent variable (observations x 1).\n",
    "    \n",
    "    Returns:\n",
    "    - The sum of squared residuals.\n",
    "    \"\"\"\n",
    "    # Predicted values from the model\n",
    "    y_pred = X * beta\n",
    "    # Calculate residuals\n",
    "    residuals = y - y_pred\n",
    "    # Sum of squared residuals\n",
    "    ssr = np.sum(residuals**2)\n",
    "    return ssr\n",
    "\n",
    "def estimate_ols(y: np.array, X: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Estimate OLS coefficients for the given data without using the closed-form solution.\n",
    "    \n",
    "    Parameters:\n",
    "    - y: np.array, the dependent variable (1000 x 1).\n",
    "    - X: np.array, the independent variables (1000 x 3).\n",
    "    \n",
    "    Returns:\n",
    "    - A np.array with the estimated coefficients (4 x 1), including the intercept.\n",
    "    \"\"\"\n",
    "    # Augment X with a column of ones for the intercept term\n",
    "    X_with_intercept = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "    # Initial guess for the coefficients (intercept + coefficients for each feature)\n",
    "    initial_guess = np.zeros(X_with_intercept.shape[1])\n",
    "    # Minimize the sum of squared residuals\n",
    "    result = sp.optimize.minimize(fun=sum_of_squared_residuals, x0=initial_guess, args=(X_with_intercept, y))\n",
    "    \n",
    "    return np.round(result.x, decimals = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b1bf6fc5-06f3-49f1-bd41-ad96cc7b2d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 3., 2., 6.])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z, t = simulate_data(481)\n",
    "estimate_ols(z, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3f559e-48ac-492a-8cc9-de02b2a93846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa4421a-a321-40b3-a07f-e452cfd465b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055a3eff-ecc1-429b-921a-9b04a577df67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff9602-4221-4c76-bdf9-df865ee51cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_likelihood(beta, X, y):\n",
    "    \"\"\"\n",
    "    Compute the negative log-likelihood for the linear model.\n",
    "    \n",
    "    Parameters:\n",
    "    - beta: Coefficients (including intercept) for the linear model.\n",
    "    - X: Independent variables (1000 x 3 np.array).\n",
    "    - y: Dependent variable (1000 x 1 np.array).\n",
    "    \n",
    "    Returns:\n",
    "    - The negative log-likelihood of the model given the data.\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    # Include intercept in the model\n",
    "    X_with_intercept = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "    # Predicted values\n",
    "    y_pred = X_with_intercept * beta\n",
    "    # Calculate residuals\n",
    "    residuals = y - y_pred\n",
    "    # Assuming epsilon ~ N(0,1), calculate negative log-likelihood\n",
    "    neg_log_like = 0.5 * n * np.log(2 * np.pi) + 0.5 * np.sum(residuals ** 2)\n",
    "    return neg_log_like\n",
    "\n",
    "def estimate_mle_parameters(y, X):\n",
    "    \"\"\"\n",
    "    Estimates the MLE parameters for a linear model using optimization.\n",
    "    \n",
    "    Parameters:\n",
    "    - y: Dependent variable (1000 x 1 np.array).\n",
    "    - X: Independent variables (1000 x 3 np.array).\n",
    "    \n",
    "    Returns:\n",
    "    - A 4 x 1 np.array with the coefficients (beta_0, beta_1, beta_2, beta_3).\n",
    "    \"\"\"\n",
    "    # Initial guess for the coefficients (including intercept)\n",
    "    initial_guess = np.zeros(X.shape[1] + 1)\n",
    "    # Minimize the negative log-likelihood\n",
    "    result = sp.optimize.minimize(fun=neg_log_likelihood, x0=initial_guess, args=(X, y), method = 'Nelder-Mead')\n",
    "    # Return the estimated parameters\n",
    "    return result.x\n",
    "\n",
    "z, t = simulate_data(481)\n",
    "print(estimate_mle_parameters(z, t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
